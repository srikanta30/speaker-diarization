# -*- coding: utf-8 -*-
"""Speaker Diarization & Speaker Identification Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mY_XYkXT5aFdx0MCJymdZFzMPLwCY3MP
"""

!pip install --q git+https://github.com/m-bain/whisperx.git

!pip install --upgrade pyannote.audio

import whisperx
import gc

device = "cpu"
batch_size = 16
compute_type = "float32"

audio_file = "conversation.wav"

import os
print(os.getcwd())  # Print the current working directory
print(os.path.exists(audio_file))  # Check if the file exists at the specified path

model = whisperx.load_model("large-v2", device , compute_type = compute_type)

audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)
print(result["segments"])

result

# Align whisper output
model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

print(result["segments"])  #after alignment

result

from google.colab import userdata
diarize_model = whisperx.DiarizationPipeline(use_auth_token=userdata.get('HF_TOKEN'), device=device)

diarize_segments = diarize_model(audio, min_speakers= 2, max_speakers=2)

diarize_segments

diarize_segments.speaker.unique()

!pip install pydub

import pydub

# Load the audio file
audio = pydub.AudioSegment.from_wav(audio_file)

# Get unique speakers
speakers = diarize_segments.speaker.unique()

# Iterate over speakers
for speaker in speakers:
    # Filter segments for the current speaker
    speaker_segments = diarize_segments[diarize_segments['speaker'] == speaker]

    # Initialize an empty audio segment for the speaker
    merged_audio = pydub.AudioSegment.empty()

    # Iterate over speaker segments and merge
    for i in range(len(speaker_segments)):
        start = speaker_segments.iloc[i]['start']
        end = speaker_segments.iloc[i]['end']

        segment = audio[start * 1000 : end * 1000]
        merged_audio += segment

    # Save the merged audio for the speaker
    merged_audio.export(f"merged_{speaker}.wav", format="wav")

!pip install pyannote.audio torch torchaudio

import torchaudio
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
from scipy.spatial.distance import cdist


def load_audio(file_path):
    waveform, sample_rate = torchaudio.load(file_path)
    return waveform, sample_rate

def extract_embedding(audio, sample_rate):
    embedding_model = PretrainedSpeakerEmbedding("speechbrain/spkrec-ecapa-voxceleb")
    waveform1, sample_rate1 = audio, sample_rate
    return embedding_model(waveform1[None])

def compare_embeddings(embedding1, embedding2):
    # Compute cosine similarity
    similarity = 1 - cdist(embedding1, embedding2, metric="cosine")
    return similarity[0][0]

def main(suspect_file, other_file):
    # Load audio files
    suspect_audio, suspect_sr = load_audio(suspect_file)
    other_audio, other_sr = load_audio(other_file)

    # Ensure sample rates are the same
    if suspect_sr != other_sr:
        raise ValueError("Sample rates of the audio files do not match")

    # Extract embeddings
    suspect_embedding = extract_embedding(suspect_audio, suspect_sr)
    other_embedding = extract_embedding(other_audio, other_sr)

    # Compare embeddings
    similarity = compare_embeddings(suspect_embedding, other_embedding)

    # Determine match threshold
    threshold = 0.8  # This is an example threshold; adjust based on your requirements


    if similarity >= threshold:
        print(f"Matched as the percentage of voice matched is: {similarity * 100:.2f}%")
    else:
        print(f"Not Matched as the percentage of voice matched is: {similarity * 100:.2f}%")

# Example usage
suspect_file = "speaker_2b.wav"
other_file = "merged_SPEAKER_00.wav"
main(suspect_file, other_file)